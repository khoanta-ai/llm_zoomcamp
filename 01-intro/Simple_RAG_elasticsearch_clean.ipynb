{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b97917-b310-4d38-9747-ddede607e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-20 06:23:52--  https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/documents.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 658332 (643K) [text/plain]\n",
      "Saving to: ‘documents.json.1’\n",
      "\n",
      "documents.json.1    100%[===================>] 642.90K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2024-06-20 06:23:53 (101 MB/s) - ‘documents.json.1’ saved [658332/658332]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Download documents.json\n",
    "# !wget https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/documents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30527a8-0da1-4426-82d7-7833c29352c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f2d09bd-61a6-418e-8f26-962cfdf68b81",
   "metadata": {},
   "source": [
    "![rag_chart.png](./imgs/rag_chart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dce00bc-1ab0-4abf-b826-ae355dc34524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_elastic_search():\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"text\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"} \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return index_settings\n",
    "\n",
    "\n",
    "def build_search_query(num_results, query, text_boost_fields, query_type, filter_dict):\n",
    "    search_query = {\n",
    "        \"size\": num_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": text_boost_fields,\n",
    "                        \"type\": query_type\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": filter_dict\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return search_query\n",
    "\n",
    "# https://techoverflow.net/2021/08/04/how-to-fix-elasticsearch-exceptions-requesterror-requesterror400-resource_already_exists_exception-index-already-exists-in-python/\n",
    "def es_create_index_if_not_exists(es, index_name, body, documents):\n",
    "    \"\"\"Create the given ElasticSearch index and ignore error if it already exists\"\"\"\n",
    "    try:\n",
    "        es.indices.create(index=index_name, body=body)\n",
    "        for doc in tqdm(documents):\n",
    "            es.index(index=index_name, document=doc)\n",
    "    except elasticsearch.exceptions.RequestError as ex:\n",
    "        if ex.error == 'resource_already_exists_exception':\n",
    "            pass # Index already exists. Ignore.\n",
    "        else: # Other exception - raise it\n",
    "            raise ex\n",
    "\n",
    "def build_elastic_search(elasticsearch_url, documents, index_name=\"course-questions\"):\n",
    "    index_settings = config_elastic_search()\n",
    "    es_client = Elasticsearch(elasticsearch_url) \n",
    "    # es_client.indices.create(index=index_name, body=index_settings)\n",
    "    es_create_index_if_not_exists(es=es_client, index_name=index_name, \n",
    "                                  body=index_settings, documents=documents)\n",
    "    return es_client\n",
    "\n",
    "def elastic_search(index_name, elastic_query, es_client):\n",
    "    response = es_client.search(index=index_name, body=elastic_query)\n",
    "    result_docs = []\n",
    "    for hit in tqdm(response['hits']['hits']):\n",
    "        result_docs.append(hit['_source'])\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746de48a-057b-43f3-9819-39230b19254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents_from_json(json_path):\n",
    "    '''\n",
    "    Convert document json format to a list of\n",
    "    elements which contain 4 objects: \n",
    "    course, text, question, and section\n",
    "    '''\n",
    "    with open(json_path, 'rt') as f_in:\n",
    "        docs_raw = json.load(f_in) \n",
    "    documents = []\n",
    "    \n",
    "    for course_dict in docs_raw:\n",
    "        for doc in course_dict['documents']:\n",
    "            doc['course'] = course_dict['course']\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "QUESTION: {question}\n",
    "CONTEXT: {context}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_llm(base_url, api_key):\n",
    "    client = OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "def query_llm(prompt, client, model_name):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{'role':'user', 'content':prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c50f253-1101-4b25-8ad1-ea253e650b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c12de24-4953-4a14-90b9-929d4c53a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 158275.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 ms, sys: 6.41 ms, total: 37.5 ms\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = 'how do I run kafka?'\n",
    "\n",
    "json_doc_path = 'documents.json'\n",
    "cvt_documents = build_documents_from_json(json_doc_path)\n",
    "# print(cvt_documents)\n",
    "\n",
    "elasticsearch_url = 'http://localhost:9200'\n",
    "index_name = \"course-questions2\"\n",
    "es_client = build_elastic_search(elasticsearch_url, cvt_documents, index_name)\n",
    "\n",
    "num_results = 10\n",
    "text_boost_fields = [\"question^3\", \"text\", \"section\"]\n",
    "query_type = \"best_fields\"\n",
    "# keyword_fields = [\"course\"]\n",
    "filter_dict = {'course': 'data-engineering-zoomcamp'}\n",
    "elastic_query = build_search_query(num_results=num_results, query=query, \n",
    "                                       text_boost_fields=text_boost_fields,\n",
    "                                       query_type=query_type,filter_dict=filter_dict)\n",
    "elastic_results = elastic_search(index_name, elastic_query, es_client)\n",
    "\n",
    "prompt = build_prompt(query=query, search_results=elastic_results)\n",
    "base_url = 'http://localhost:11434/v1/'\n",
    "api_key = 'ollama'\n",
    "model_name = 'phi3'\n",
    "phi3_client = build_llm(base_url, api_key)\n",
    "response_res = query_llm(prompt=prompt, client=phi3_client, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28cdf43e-f255-46b9-a08c-25ece504119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To run Kafka in the context provided by the course database, it seems there is no direct information regarding executing a Kafka producer or consumer from the FAQs available. However, based on similar workflows and general practices for setting up a Kafka environment:\n",
      "\n",
      "\n",
      "1. Ensure that your development environment has Java installed as Kafka requires it to run its producers/consumers.\n",
      "\n",
      "2. Install Python if not already done since we'll use the `kafka-python` package, which can be installed via pip (`pip install kafka-python`).\n",
      "\n",
      "3. Download or clone the Kafka source code from GitHub (https://github.com/apache/kafka). This step is necessary to build and run the native Java components of Kafka with your local environment if needed.\n",
      "\n",
      "4. Navigate into the directory containing `build/` and use the command as mentioned in the context:\n",
      "\n",
      "```bash\n",
      "java -cp build/libs/*-1.0.SNAPSHOT.jar:out src/main/java/org/apache/kafka/examples/KafkaServerApplication.class\n",
      "```\n",
      "\n",
      "This command compiles and runs a sample Kafka server application that starts the broker on port 9092. You can adjust this by locating the correct Kafka version you want to run or modify it based on your project requirements.\n",
      "\n",
      "Keep in mind, if there's specific Python code for interacting with Kafka (producer or consumer scripts), that would be executed separately using a Python environment set up as per the information given earlier about virtual environments and dependencies management.\n"
     ]
    }
   ],
   "source": [
    "print(response_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a263d073-6fe2-4bc3-b2bb-638b17a754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_rag(query):\n",
    "    elastic_query = build_search_query(num_results=num_results, query=query, \n",
    "                                       text_boost_fields=text_boost_fields,\n",
    "                                       query_type=query_type,filter_dict=filter_dict)\n",
    "    elastic_results = elastic_search(index_name, elastic_query, es_client)\n",
    "    prompt = build_prompt(query=query, search_results=elastic_results)\n",
    "    response_res = query_llm(prompt=prompt, client=phi3_client, model_name=model_name)\n",
    "    return response_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c082702-e482-42e3-8283-3d79b8d6db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 59747.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Yes, even after the start date of the course, you can still join by submitting homeworks as long as there is time left for the final project deadlines. However, it's recommended not to leave everything for the last minute.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(elastic_rag(query= 'the course has already started, can I still enroll?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9d6bd-6bfd-46c6-9d0a-5646717fd686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
